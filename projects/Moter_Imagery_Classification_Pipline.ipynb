{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Moter Imagery Classification-Pipline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPI1HFtIxHV7i+kC5oN7m/v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TugaAhmed/ECoG_motor_imagery/blob/main/projects/Moter_Imagery_Classification_Pipline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install/Import Libraries \n",
        "\n",
        "!pip install EntropyHub --quiet\n",
        "!pip install nilearn --quiet\n",
        "!pip install nimare --quiet\n",
        "!pip install duecredit --quiet\n",
        "\n",
        "from matplotlib import rcParams\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "rcParams['figure.figsize'] = [20, 4]\n",
        "rcParams['font.size'] = 15\n",
        "rcParams['axes.spines.top'] = False\n",
        "rcParams['axes.spines.right'] = False\n",
        "rcParams['figure.autolayout'] = True\n",
        "\n",
        "import numpy as np\n",
        "from scipy.stats import entropy as en\n",
        "import scipy.stats as scst\n",
        "import EntropyHub as enh\n",
        "\n",
        "\n",
        "import pickle as pickle\n",
        "import pandas as pd\n",
        "import math\n",
        "\n",
        "from sklearn import svm\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "import os\n",
        "import time\n",
        "import statistics as st\n",
        "import scipy\n",
        "from scipy.fftpack import *\n",
        "from scipy.signal import hilbert, chirp\n",
        "from scipy import signal\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "fkU3qP4ouGVG"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Motor Imagery Classification With Optimal Selection Of Minimum Electrode Subsets**\n",
        "\n",
        "Group project developed by Chrysalis team, NMA 2022\n",
        "\n",
        "**Pipeline :**\n",
        "\n",
        "\n",
        "1. Data acquisition \n",
        "2. Data filtering/artifact removal\n",
        "3. Epoching and Down-sampling\n",
        "4. Feature Selection \n",
        "5. Model Fitting \n",
        "6. Channel Selection\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "K0THE1PSskJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Feature Selection Methods\n",
        "\n",
        "def ERD_feature(trial_task, trial_rest) :\n",
        "  \n",
        "  # Step1 : high-pass filter (above 8 Hz)\n",
        "  b1, a1 = signal.butter(3, [8], btype='high', fs=1000)\n",
        "  trial_rest_high_filtered = signal.filtfilt(b1, a1, trial_rest, 0)\n",
        "\n",
        "  b2, a2 = signal.butter(3, [8], btype='high', fs=1000)\n",
        "  trial_task_high_filtered = signal.filtfilt(b2, a2, trial_task, 0)\n",
        "\n",
        "  # Step2 : low-pass filter (below 12 Hz)\n",
        "  b1, a1 = signal.butter(3, [12], btype='low', fs=1000)\n",
        "  trial_rest_bandpass_filtered = signal.filtfilt(b1, a1, trial_rest, 0)\n",
        "\n",
        "  b2, a2 = signal.butter(3, [12], btype='low', fs=1000)\n",
        "  trial_task_bandpass_filtered = signal.filtfilt(b2, a2, trial_task, 0)\n",
        "  \n",
        "  # Same time_window size for trial_task and trial_rest(can be different )\n",
        "  time_window = trial_task.shape[0]  # One second or less for BCI application\n",
        "  \n",
        "  # Step3 : power_spectrum \n",
        "  rest_fourier_transform = np.fft.rfft(trial_rest_bandpass_filtered)\n",
        "  rest_abs_fourier_transform = np.abs(rest_fourier_transform)\n",
        "  rest_power_spectrum = np.square(rest_abs_fourier_transform)\n",
        "\n",
        "  task_fourier_transform = np.fft.rfft(trial_task_bandpass_filtered)\n",
        "  task_abs_fourier_transform = np.abs(task_fourier_transform)\n",
        "  task_power_spectrum = np.square(task_abs_fourier_transform)\n",
        "\n",
        "  # Step4: \n",
        "  # P_rest is the mean power spectra during the rest period \n",
        "  # P_task is the mean power spectra during the task period \n",
        "\n",
        "  p_rest = (1/time_window) * np.sum(rest_power_spectrum)\n",
        "  p_task = (1/time_window) * np.sum(task_power_spectrum)\n",
        "\n",
        "  # Step5: relative power (RP)\n",
        "\n",
        "  RP = ((p_task - p_rest) / p_rest ) * 100\n",
        "  \n",
        "  return RP\n",
        "\n",
        "\n",
        "\n",
        "def shannon_entropy_feature(trial) :\n",
        "  square = np.power(trial,2)\n",
        "  square_sum = np.sum(square)\n",
        "  # energy Probability\n",
        "  EP = square/square_sum\n",
        "  # Shannon entropy\n",
        "  SE = -np.sum(EP * np.log(np.power(EP,2)))\n",
        "  return SE\n",
        "\n",
        "\n",
        "\n",
        "def log_energy_entropy_feature(trial) :\n",
        "  square = np.power(trial,2)\n",
        "  square_sum = np.sum(square)\n",
        "  # energy Probability\n",
        "  EP = square/square_sum\n",
        "  # LogEnergy entropy\n",
        "  LEE = np.sum(EP * np.log(EP))\n",
        "  return LEE\n",
        "\n",
        "\n",
        "\n",
        "def hilbert_feature(trial) :\n",
        "  analytic_signals = scipy.signal.hilbert(trial)\n",
        "  amplitude_envelope = np.abs(analytic_signals)\n",
        "  averaged_amplitude_envelope = amplitude_envelope.mean()\n",
        "  return averaged_amplitude_envelope\n",
        "\n",
        "\n",
        "def log_variance_feature(trial) :\n",
        "  log_variance = np.log(np.var(trial))\n",
        "  return log_variance\n",
        "\n",
        "\n",
        "def std_feature(trial) :\n",
        "  return np.std(trial)\n",
        "\n",
        "\n",
        "def mean_feature(trial) :\n",
        "  return np.mean(trial)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "_AvKt-AL46K5"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "id": "bilaemYWsc1h"
      },
      "outputs": [],
      "source": [
        "#@title 1. Data acquisition\n",
        "# 1. Data acquisition\n",
        "import os, requests\n",
        "\n",
        "fname = 'motor_imagery.npz'\n",
        "url = \"https://osf.io/ksqv8/download\"\n",
        "\n",
        "if not os.path.isfile(fname):\n",
        "  try:\n",
        "    r = requests.get(url)\n",
        "  except requests.ConnectionError:\n",
        "    print(\"!!! Failed to download data !!!\")\n",
        "  else:\n",
        "    if r.status_code != requests.codes.ok:\n",
        "      print(\"!!! Failed to download data !!!\")\n",
        "    else:\n",
        "      with open(fname, \"wb\") as fid:\n",
        "        fid.write(r.content)\n",
        "\n",
        "alldat = np.load(fname, allow_pickle=True)['dat']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2.Data filtering/artifact removal\n",
        "\n",
        "def filter_EcoG_data(subject_number = 0 , experiment_number = 1) :\n",
        "  '''\n",
        "       input : \n",
        "        1. subject number - int value\n",
        "        2. experiment number - int value\n",
        "       return :\n",
        "        filtred Voltage np array\n",
        "  '''\n",
        "  data = alldat[subject_number][experiment_number]  # Data from  first subject , second experiment(motor imagery)\n",
        "\n",
        "  # V is the voltage data\n",
        "  V = data['V'].astype('float32')\n",
        "\n",
        "  # high-pass filter above 0.1 Hz\n",
        "  b, a = signal.butter(3, [0.1], btype='high', fs=1000)\n",
        "  V = signal.filtfilt(b, a, V, 0)\n",
        "\n",
        "  # low-pass filter above 45 Hz\n",
        "  b, a = signal.butter(3, [45], btype='low', fs=1000)\n",
        "  V = signal.filtfilt(b, a, V, 0)\n",
        "\n",
        "\n",
        "  # Work on Notch Filtering @ 60 HZ\n",
        "  fs = 1000.0  # Sample frequency (Hz)\n",
        "  f0 = 60.0  # Frequency to be removed from signal (Hz)\n",
        "  Q = 2.0  # Quality factor\n",
        "  # Design notch filter\n",
        "  b, a = signal.iirnotch(f0, Q, fs)\n",
        "  NotchSignal60 = signal.filtfilt(b, a, V)\n",
        "\n",
        "\n",
        "  # Work on Notch Filtering @ 120 HZ\n",
        "  fs = 1000.0  # Sample frequency (Hz)\n",
        "  f2 = 120.0  # Frequency to be removed from signal (Hz)\n",
        "  Q = 2.0  # Quality factor\n",
        "  # Design notch filter\n",
        "  b, a = signal.iirnotch(f0, Q, fs)\n",
        "  NotchSignal120 = signal.filtfilt(b, a, V)\n",
        "\n",
        "\n",
        "  # normalize each channel so its mean power is 1\n",
        "  V = V/V.mean(0)\n",
        "  return V\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "XT1LyQOxz-Yv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3.Epoching and Down-Sampling\n",
        "\n",
        "\n",
        "\n",
        "def epoching_down_sampling(filtered_V,subject_number=0,experiment_number=1,\\\n",
        "                           channels_list=[1] , sample_size = 1000) :\n",
        "  '''\n",
        "     input : \n",
        "         filtered voltage data - np array\n",
        "         subject number - int\n",
        "         experiment number - int\n",
        "         channels list - list\n",
        "         sample size  - int\n",
        "    return :\n",
        "         stacked array of all channels \n",
        "         1D array of labels \n",
        "  '''\n",
        "  def get_labels_array(channels_list, subject_number , experiment_number) :\n",
        "    stacked_labels = np.zeros(60*len(channels_list))\n",
        "    \n",
        "    for channel in range(len(channels_list)) :\n",
        "      new_stim_id_array = np.zeros(60)\n",
        "      for  i , stim_id in enumerate(alldat[subject_number][experiment_number]['stim_id']) :\n",
        "        if stim_id == 11 :\n",
        "          new_stim_id_array[i] = 0\n",
        "        else :\n",
        "          new_stim_id_array[i] = 1\n",
        "      stacked_labels[60*channel:(60*channel) + 60 ] = new_stim_id_array\n",
        "    return stacked_labels\n",
        "\n",
        "\n",
        "  V_T = filtered_V.T\n",
        "  channels_array_list = []\n",
        "  t_on_array = alldat[subject_number][experiment_number]['t_on']\n",
        "  t_off_array = alldat[subject_number][experiment_number]['t_off']\n",
        "  step_size = (t_off_array[0] - t_on_array[0]) / sample_size\n",
        "\n",
        "  for channel in channels_list :\n",
        "    channel_array = V_T[channel][t_on_array[0] : t_off_array[0]-1][::int(step_size)]\n",
        "    for t_on , t_off  in zip (t_on_array[1:] , t_off_array[1:]) :\n",
        "      slice_v = V_T[channel][t_on : t_off-1][::int(step_size)]\n",
        "      channel_array = np.vstack((channel_array,slice_v))\n",
        "    channels_array_list.append(channel_array)\n",
        "  \n",
        "  all_channels_stacked_array = np.concatenate(channels_array_list , axis=0)\n",
        "\n",
        "  labels = get_labels_array(channels_list , subject_number , experiment_number) \n",
        "  return all_channels_stacked_array , labels\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "2L7GOsPz1vGU"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 4.Feature Selection\n",
        "\n",
        "def feature_selection(all_channels_stacked_array) :\n",
        "  '''\n",
        "    Convert raw V to features \n",
        "    input :\n",
        "      all channels stacked array\n",
        "    return :\n",
        "      X array(number of samples , number of features) as input to classifer\n",
        "  '''\n",
        "\n",
        "  # ERD_feature\n",
        "  number_of_samples = all_channels_stacked_array.shape[0]\n",
        "  mean_array = std_array = log_variance_array = hilbert_array =\\\n",
        "  log_energy_entropy_array = shannon_entropy_array = np.zeros(number_of_samples)\n",
        "\n",
        "  for i , time_window in enumerate(all_channels_stacked_array) :\n",
        "    mean_array[i] = mean_feature(time_window)\n",
        "    std_array[i] = std_feature(time_window)\n",
        "    log_variance_array[i] = log_variance_feature(time_window)\n",
        "    hilbert_array[i] = hilbert_feature(time_window)\n",
        "    log_energy_entropy_array[i] = log_energy_entropy_feature(time_window)\n",
        "    shannon_entropy_array[i] = shannon_entropy_feature(time_window)\n",
        "    \n",
        "\n",
        "\n",
        "  # Features array as X input to classifer\n",
        "  X = np.column_stack((mean_array,std_array,log_variance_array,hilbert_array\\\n",
        "                       ,log_energy_entropy_array,shannon_entropy_array))\n",
        "  return X\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "X2CwhuEYyiNP"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "V = filter_EcoG_data(subject_number=0,experiment_number=1)\n",
        "V.shape"
      ],
      "metadata": {
        "id": "Xe6pwuy16LuB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67deb714-42ec-491a-c663-7deeb220b853"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(376600, 46)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_channels_stacked_array , labels= epoching_down_sampling(V,channels_list = [2] ,sample_size=1000)\n",
        "all_channels_stacked_array.shape"
      ],
      "metadata": {
        "id": "C2g3V4nd72mA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49ff98a1-970f-47b9-ef2f-1a33fba5fa57"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60, 1000)"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_G3vIqC5t8a",
        "outputId": "8dae3f91-64b7-41cd-d763-016f9382b485"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60,)"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = feature_selection(all_channels_stacked_array)\n",
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUQIWDVzgVfy",
        "outputId": "86997e08-0c5e-44a9-8577-d19cc047972e"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y = labels\n",
        "Y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJdVi67OkqCe",
        "outputId": "fce568bd-22f6-471f-94c9-397fc8eef123"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60,)"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5/ Model Fitting\n",
        "\n",
        "# step(1):  devide X,Y into train - test set \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.20, random_state = 101)"
      ],
      "metadata": {
        "id": "QT4s1zld9AEr"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SVM_classifer = svm.SVC(gamma=0.001 , C=100 , kernel='linear')\n",
        "SVM_classifer.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BU3vw1HQ9c3t",
        "outputId": "e34458d6-f061-4714-e8d6-3f7f271fdf62"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=100, gamma=0.001, kernel='linear')"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = SVM_classifer.predict(X_test)"
      ],
      "metadata": {
        "id": "0Z8bR_WX-h6T"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test , y_pred) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkaIa1aV-4pz",
        "outputId": "464d347a-d02f-49f3-d84a-264dba21045a"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.36      1.00      0.53         4\n",
            "         1.0       1.00      0.12      0.22         8\n",
            "\n",
            "    accuracy                           0.42        12\n",
            "   macro avg       0.68      0.56      0.38        12\n",
            "weighted avg       0.79      0.42      0.33        12\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RTmqcItc-_kQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}